[int]
trg_max_len = 2048
n_heads = 12
n_kv_heads = 1
d_model = 1536
d_ff = 4096
n_dec_layers = 24
trg_vocab_size = 100000
max_rope_len = 2048 
max_decode_steps = 1000
#top_k = 20
beam_size = 1
repetition_window_size = 30

[float]
#dropout = 0
ln_eps = 1e-12
top_p = 0.85
temperature = 0.97
repetition_penalty = -0.1

[str]
trg_vocab = app/model/commentMain/model/vocab/mimix_vocab.txt
#load_model = model/tmp.test902.4.model
load_model = app/model/commentMain/model/tempComment/MimixLM-0.7b-sft.model
search_strategy = sample
#search_strategy = beam_search
trg_tokenizer = mimix-cased
activation = swish 
layer_norm_type = rms_norm
model = transformer
task = lm

[bool]
use_cuda = False
#use_cuda = True
is_mimix_chat = True
share_emb_out_proj = True
use_pre_norm = True
norm_before_pred = True
use_pos_embedding = False
use_rope_embedding = True
use_glu = True
use_attention_bias = False
use_ffn_bias = False
use_ln_bias = False
use_output_bias = False
